{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6caacd",
   "metadata": {},
   "source": [
    "Date: 01/06/2025 <br>\n",
    "Author: Wan Xuen <br>\n",
    "Notebook01: Text Mining for Mental Health Chatbot <br>\n",
    "Aim: To conduct text mining and save the lemmatized texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b7ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import spacy\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm  # Progress bar for feedback\n",
    "\n",
    "FILE_DIR = 'raw data'\n",
    "DIC_DIR = 'dictionary'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037a8d7",
   "metadata": {},
   "source": [
    "### Read all files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8596e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concat_all_csvs(base_path):\n",
    "    all_dfs = []\n",
    "\n",
    "    # Loop over years (folders under base_path)\n",
    "    for year in sorted(os.listdir(base_path)):\n",
    "        year_path = os.path.join(base_path, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue  # Skip non-folder files\n",
    "\n",
    "        # Loop over months inside each year\n",
    "        for month in sorted(os.listdir(year_path)):\n",
    "            month_path = os.path.join(year_path, month)\n",
    "            if not os.path.isdir(month_path):\n",
    "                continue\n",
    "\n",
    "            # Loop over CSV files inside each month\n",
    "            for filename in sorted(os.listdir(month_path)):\n",
    "                if filename.endswith('.csv'):\n",
    "                    file_path = os.path.join(month_path, filename)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No CSV files found.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e25340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389387, 8)\n"
     ]
    }
   ],
   "source": [
    "df_final = read_and_concat_all_csvs(FILE_DIR)\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9633e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 389387 entries, 0 to 389386\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Unnamed: 0   389387 non-null  int64 \n",
      " 1   author       389387 non-null  object\n",
      " 2   created_utc  389387 non-null  int64 \n",
      " 3   score        389387 non-null  int64 \n",
      " 4   selftext     378246 non-null  object\n",
      " 5   subreddit    389387 non-null  object\n",
      " 6   title        389387 non-null  object\n",
      " 7   timestamp    389387 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 23.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41d619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=[\"Unnamed: 0\", \"timestamp\", \"score\", \"created_utc\", \"author\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5070388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selftext     11141\n",
       "subreddit        0\n",
       "title            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366a5cc",
   "metadata": {},
   "source": [
    "### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b21faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['selftext']=df_final['selftext'].fillna('N/A')\n",
    "df_final['title']=df_final['title'].fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db1b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['full_text'] = (\n",
    "    \"Subreddit: \" + df_final['subreddit'].fillna('') + \". \" +\n",
    "    \"Title: \" + df_final['title'].fillna('') + \". \" +\n",
    "    \"Body: \" + df_final['selftext'].fillna('')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea935bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 389387 entries, 0 to 389386\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   selftext   389387 non-null  object\n",
      " 1   subreddit  389387 non-null  object\n",
      " 2   title      389387 non-null  object\n",
      " 3   full_text  389387 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 11.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b0b71",
   "metadata": {},
   "source": [
    "### Clean the text\n",
    "- lowercasing\n",
    "- URLs, punctuation, emojis, extra space, user mentions removal\n",
    "- replace newlines with space\n",
    "- repeated words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07306ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\n', ' ', text)      # replace newlines with space\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation and special chars, keep letters and digits\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    text = re.sub(r'u\\/\\w+|@\\w+', '', text) # remove user mentions\n",
    "    # Remove emojis (unicode ranges)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002700-\\U000027BF\"  # dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub(r'&\\w+;', '', text) # Remove HTML entities like &amp;, &gt;, &lt;, etc.\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)  # \"soooo\" â†’ \"soo\"\n",
    "    return text\n",
    "\n",
    "df_final['clean_text'] = df_final['full_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1b669",
   "metadata": {},
   "source": [
    "### Short forms or slang customly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2248827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DIC_DIR, \"slang_dictionary.json\"), \"r\") as f:\n",
    "    slang_dict = json.load(f)\n",
    "\n",
    "def expand_slang(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([slang_dict.get(w, w) for w in words])\n",
    "\n",
    "df_final['expanded_text'] = df_final['clean_text'].apply(expand_slang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd50df",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9887e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "spell_dict = \"frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell.load_dictionary(os.path.join(DIC_DIR, spell_dict), term_index=0, count_index=1)\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_correct(word):\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else word\n",
    "\n",
    "def correct_spelling_fast_cached(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([cached_correct(w) for w in words])\n",
    "\n",
    "df_final['corrected_text'] = df_final['expanded_text'].astype(str).apply(correct_spelling_fast_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e76ef",
   "metadata": {},
   "source": [
    "Note: Use SymSpell only if you detect gibberish or low confidence in production chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305905d",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "To reduce dimensionality or group similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daeac0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 0: 100000it [15:40, 106.36it/s]\n",
      "Chunk 100000: 100000it [14:34, 114.30it/s]\n",
      "Chunk 200000: 100000it [14:20, 116.25it/s]\n",
      "Chunk 300000: 89387it [13:14, 112.56it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "df_final['corrected_text'] = df_final['corrected_text'].astype(str)\n",
    "\n",
    "lemmatized_col = []\n",
    "chunk_size = 100_000\n",
    "\n",
    "for i in range(0, len(df_final), chunk_size):\n",
    "    chunk = df_final['corrected_text'].iloc[i:i+chunk_size].tolist()\n",
    "    partial_result = [\n",
    "        \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
    "        for doc in tqdm(nlp.pipe(chunk, batch_size=1000, n_process=2), desc=f\"Chunk {i}\")\n",
    "    ]\n",
    "    lemmatized_col.extend(partial_result)\n",
    "\n",
    "df_final['lemmatized_text'] = lemmatized_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1ce56",
   "metadata": {},
   "source": [
    "### Save lemmatized text into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ad6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_parquet('processing.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
