{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911c34c5",
   "metadata": {},
   "source": [
    "Date: 12/07/2025 <br>\n",
    "Author: Wan Xuen <br>\n",
    "Notebook03: Text Mining for Mental Health Chatbot <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353f05bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\ey\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from typing import TypedDict, Annotated, List\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import BaseMessage\n",
    "from langdetect import detect, LangDetectException\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_fireworks import ChatFireworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc4b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3800\\2513972482.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_func = SentenceTransformerEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
      "INFO:datasets:PyTorch version 2.7.1+cu118 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3800\\2513972482.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "embedding_func = SentenceTransformerEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embedding_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a707de",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc583372",
   "metadata": {},
   "source": [
    "### Use PortKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e43acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5660ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key= GROQ_API_KEY,\n",
    "    base_url=PORTKEY_GATEWAY_URL,\n",
    "    default_headers=createHeaders(\n",
    "        provider=\"groq\",\n",
    "        api_key= PORTKEY_API_KEY, \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505f4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000016F9CA87D90>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000016F9CA87110>\n"
     ]
    }
   ],
   "source": [
    "llm = ChatFireworks(\n",
    "    model=\"accounts/fireworks/models/llama-v3p3-70b-instruct\",\n",
    "    temperature=0.7,\n",
    "    api_key=FIREWORKS_API_KEY,  # Or use os.getenv(\"FIREWORKS_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4df8c",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67562ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_EMOTION = \"\"\"\n",
    "You are a kind, emotionally aware assistant designed to have deep and thoughtful conversations about both happy and difficult topics.\n",
    "\n",
    "You specialize in mental health, but you can also engage in uplifting, meaningful discussions.\n",
    "\n",
    "Do not assume the user is in distress unless there is clear emotional language or context. Respond based on the emotional tone of the query.\n",
    "\n",
    "If someone says hello or shares good news, respond with warmth and curiosity.\n",
    "\n",
    "If the user expresses emotional pain, distress, or mentions self-harm or depression, prioritize emotional validation and support.\n",
    "\n",
    "If the context is unclear or missing, politely ask for more information.\n",
    "\n",
    "If the context is unclear or insufficient, you may also respond with:\n",
    "\"I'm sorry, I don't have enough information to answer that based on the context provided.\"\n",
    "\n",
    "Answer in a way that is engaging, informative, and sounds natural — not robotic.\n",
    "\"\"\"\n",
    "\n",
    "tone_map = {\n",
    "    \"distressed\": \"Use a gentle, calming tone. Validate their emotions.\",\n",
    "    \"positive\": \"Respond warmly and invite the user to share more.\",\n",
    "    \"neutral\": \"Be supportive and open-ended. Encourage conversation.\",\n",
    "    \"mixed\": \"Be compassionate and inquisitive. Let the user lead.\",\n",
    "    \"empathetic\": \"Use warm and understanding language.\",\n",
    "    \"professional\": \"Keep answers concise and factual.\",\n",
    "    \"cheerful\": \"Use a positive and upbeat tone.\",\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT_GENERAL = \"\"\"\n",
    "You are a helpful, friendly, and respectful AI assistant. You answer questions clearly, accurately, and concisely.\n",
    "\n",
    "When responding:\n",
    "- If you do not have enough information, say: \"I'm sorry, I don't have enough information to answer that based on the context provided.\"\n",
    "- Always use a professional, respectful, and approachable tone.\n",
    "- Straightforwardly answer the question without unnecessary complexity.\n",
    "- For general questions (factual, technical, or everyday knowledge), respond clearly and straight to the point.\n",
    "- For technical questions, provide concise explanations and examples when needed.\n",
    "- Do not offer medical, legal, or financial advice beyond general information.\n",
    "- If unsure, encourage the user to seek professional help or provide general guidance.\n",
    "- Stay grounded, avoid assumptions, and never be dismissive.\n",
    "- Do not give so much emotional support that it distracts from the main question.\n",
    "\n",
    "Your goal is to be efficient, supportive, and direct in your answers, especially for general cases.\n",
    "Answer in a way that is engaging, informative, and sounds natural — not robotic.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d205d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def needs_emotional_support(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity < -0.5:\n",
    "        return \"strong_negative\"\n",
    "    elif polarity < -0.1:\n",
    "        return \"mild_negative\"\n",
    "    elif polarity > 0.5:\n",
    "        return \"strong_positive\"\n",
    "    elif polarity > 0.1:\n",
    "        return \"mild_positive\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555733ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BGE-Reranker\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-base\")\n",
    "\n",
    "def rerank(query, documents, top_k=3):\n",
    "    if not documents:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, doc) for doc in documents]\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        scores = model(**inputs).logits.squeeze(-1)\n",
    "\n",
    "    scored_docs = list(zip(documents, scores.tolist()))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in scored_docs[:top_k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bb49adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3800\\2226677262.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1000)\n",
    "\n",
    "@tool\n",
    "def math_tool(query: str) -> str:\n",
    "    \"\"\"A tool that calculates the result of a math expression.\"\"\"\n",
    "    try:\n",
    "        return f\"The answer is: {eval(query)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_response(query: str, system_prompt: str, memory: ConversationSummaryBufferMemory) -> str:\n",
    "    docs = vectorstore.similarity_search(query, k=10)\n",
    "    reranked = rerank(query, [d.page_content for d in docs], top_k=3)\n",
    "    context = \"\\n\\n\".join(reranked)\n",
    "    memory_summary = memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{system_prompt}\\n\\nPrevious Conversation:\\n{memory_summary}\\n\\nTone Guide: {needs_emotional_support(query)}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Using the info below:\\n{context}\\n\\nUser: {query}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    memory.chat_memory.add_user_message(query)\n",
    "    memory.chat_memory.add_ai_message(response.content)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "@tool\n",
    "def mental_health_tool(query: str) -> str:\n",
    "    \"\"\"Answer mental health-related questions using the emotion-focused system prompt.\"\"\"\n",
    "    return generate_response(query, SYSTEM_PROMPT_EMOTION,memory)\n",
    "\n",
    "@tool\n",
    "def general_tool(query: str) -> str:\n",
    "    \"\"\"Answer general questions using the general system prompt.\"\"\"\n",
    "    return generate_response(query, SYSTEM_PROMPT_GENERAL,memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df46ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3800\\4085849683.py:5: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [math_tool, mental_health_tool, general_tool]\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",# AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05eaa03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    query: Annotated[str, \"Input\"]  \n",
    "    output: Annotated[str, \"Final response\"]\n",
    "    chat_history: List[BaseMessage]\n",
    "    \n",
    "def route_tool(state: AgentState) -> str:\n",
    "    text = state[\"query\"].lower()\n",
    "\n",
    "    if \"calculate\" in text or any(x in text for x in \"+-*/=^()1234567890\"):\n",
    "        return \"math\"\n",
    "\n",
    "    mental_keywords = [\n",
    "        \"sad\", \"depressed\", \"depression\", \"unhappy\", \"lonely\", \"anxiety\", \"anxious\",\n",
    "        \"angry\", \"angriness\", \"feel lost\", \"feel empty\", \"crying\", \"want to cry\",\n",
    "        \"i feel\", \"i'm feeling\", \"panic\", \"panic attack\", \"stress\", \"stressed\",\n",
    "        \"overwhelmed\", \"helpless\", \"hopeless\", \"worthless\", \"not okay\",\n",
    "        \"mental health\", \"emotional pain\", \"broken\", \"tired of life\", \"no one cares\",\n",
    "        \"numb\", \"burnout\", \"burned out\", \"can’t cope\", \"i need help\",\n",
    "        \"therapy\", \"therapist\", \"counseling\", \"support group\", \"feel better\",\n",
    "        \"why do i feel\", \"i feel like\", \"i hate myself\", \"self harm\", \"hurt myself\",\n",
    "        \"i want to talk\", \"i need someone\", \"help me feel better\", \"is something wrong with me\",\n",
    "        \"i am not okay\", \"i'm not okay\", \"mental breakdown\", \"emotional support\",\n",
    "        \"i feel anxious\", \"i feel sad\", \"i feel down\"\n",
    "    ]\n",
    "\n",
    "    if any(keyword in text for keyword in mental_keywords):\n",
    "        return \"mental\"\n",
    "\n",
    "    return \"general\"\n",
    "\n",
    "\n",
    "def language_check_condition(state: AgentState) -> str:\n",
    "    try:\n",
    "        lang = detect(state[\"query\"])\n",
    "        if lang != \"en\":\n",
    "            if is_math_expression(state[\"query\"]):\n",
    "                return \"non_english_math\"\n",
    "            else:\n",
    "                return \"non_english\"\n",
    "        return \"english\"\n",
    "    except LangDetectException:\n",
    "        if is_math_expression(state[\"query\"]):\n",
    "            return \"non_english_math\"\n",
    "        return \"non_english\"\n",
    "\n",
    "def non_english_response(state: AgentState) -> AgentState:\n",
    "    if is_math_expression(state[\"query\"]):\n",
    "        return {\"query\": state[\"query\"], \"output\": math_tool.invoke(state[\"query\"])}\n",
    "    return {\n",
    "        \"query\": state[\"query\"],\n",
    "        \"output\": \"Sorry, this chatbot currently only supports English.\"\n",
    "    }\n",
    "\n",
    "def passthrough(state: AgentState) -> AgentState:\n",
    "    return state\n",
    "\n",
    "# Tool handlers\n",
    "def handle_math(state: AgentState) -> AgentState:\n",
    "    return {\"query\": state[\"query\"], \"output\": math_tool.invoke(state[\"query\"])}\n",
    "\n",
    "def handle_mental(state: AgentState) -> AgentState:\n",
    "    return {\"query\": state[\"query\"], \"output\": mental_health_tool.invoke(state[\"query\"])}\n",
    "\n",
    "def handle_general(state: AgentState) -> AgentState:\n",
    "    return {\"query\": state[\"query\"], \"output\": general_tool.invoke(state[\"query\"])}\n",
    "\n",
    "def error_fallback(state: AgentState) -> AgentState:\n",
    "    return {\"input\": state[\"input\"], \"output\": \"Something went wrong. Please try again later.\"}\n",
    "\n",
    "def is_math_expression(text: str) -> bool:\n",
    "    # Basic check for math-like expressions\n",
    "    return bool(re.search(r\"[\\d\\s\\+\\-\\*/\\^\\=\\(\\)]\", text))\n",
    "\n",
    "def is_short_input(text: str) -> bool:\n",
    "    # Less than 5 words, and no math operators or keywords\n",
    "    words = text.strip().split()\n",
    "    if len(words) < 5 and not re.search(r\"[\\d\\+\\-\\*/=]\", text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def input_check_condition(state: AgentState) -> str:\n",
    "    text = state[\"query\"].strip()\n",
    "    if not text or is_short_input(text):\n",
    "        return \"short_input\"\n",
    "    return \"proceed\"\n",
    "\n",
    "def short_input_handler(state: AgentState) -> AgentState:\n",
    "    text = state[\"query\"].strip()\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"unknown\"\n",
    "\n",
    "    if lang != \"en\":\n",
    "        return {\n",
    "            \"query\": state[\"query\"],\n",
    "            \"output\": \"It looks like your message is short and not in English. This assistant currently supports English only. Please try rephrasing your question in English with more context, and I’ll do my best to assist you!\"\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"query\": state[\"query\"],\n",
    "        \"output\": \"Your message seems a bit short. Could you please provide more context or details so I can better understand and help you?\"\n",
    "    }\n",
    "\n",
    "def response_node(state: dict) -> dict:\n",
    "    response = generate_response(state[\"query\"], state[\"system_prompt\"], state[\"memory\"])\n",
    "    return {\n",
    "        \"query\": state[\"query\"],\n",
    "        \"system_prompt\": state[\"system_prompt\"],\n",
    "        \"output\": response,\n",
    "        \"memory\": state[\"memory\"]\n",
    "    }\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"input_check\", passthrough)\n",
    "workflow.add_node(\"short_input\", short_input_handler)\n",
    "workflow.add_node(\"language_check\", passthrough)\n",
    "workflow.add_node(\"non_english\", non_english_response)\n",
    "workflow.add_node(\"error\", error_fallback)\n",
    "workflow.add_node(\"router\", passthrough)\n",
    "workflow.add_node(\"math\", handle_math)\n",
    "workflow.add_node(\"mental\", handle_mental)\n",
    "workflow.add_node(\"general\", handle_general)\n",
    "workflow.add_node(\"end\", lambda s: s)\n",
    "\n",
    "workflow.set_entry_point(\"input_check\")\n",
    "\n",
    "# Route short/long input\n",
    "workflow.add_conditional_edges(\"input_check\", input_check_condition, {\n",
    "    \"short_input\": \"short_input\",\n",
    "    \"proceed\": \"language_check\"\n",
    "})\n",
    "workflow.add_edge(\"short_input\", \"end\")\n",
    "\n",
    "# Language check flow\n",
    "workflow.add_conditional_edges(\"language_check\", language_check_condition, {\n",
    "    \"non_english_math\": \"math\",\n",
    "    \"non_english\": \"non_english\",\n",
    "    \"english\": \"router\"\n",
    "})\n",
    "workflow.add_edge(\"non_english\", \"end\")\n",
    "\n",
    "workflow.add_conditional_edges(\"router\", route_tool, {\n",
    "    \"math\": \"math\",\n",
    "    \"mental\": \"mental\",\n",
    "    \"general\": \"general\"\n",
    "})\n",
    "\n",
    "workflow.add_edge(\"math\", \"end\")\n",
    "workflow.add_edge(\"mental\", \"end\")\n",
    "workflow.add_edge(\"general\", \"end\")\n",
    "workflow.add_edge(\"error\", \"end\")\n",
    "workflow.set_finish_point(\"end\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad97b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so sorry to hear that you're feeling sad today. It takes a lot of courage to acknowledge and express your emotions, and I'm here to listen and support you. If you're willing, could you tell me more about what's been going on and how you're feeling? Sometimes talking through our emotions can help clarify things and gain a different perspective. What happened, and how did you feel about the results? Was it something you were hoping to do well on, or was it a surprise? I'm here to support you in exploring your emotions, and I want to help you process your feelings. By talking through what you're experiencing, including any feelings or sensations you've been having, we can work together to understand your emotions better.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "user_input = \"I am sad today because I get bad results. Can you help me understand why I feel this way?\"\n",
    "\n",
    "# Sentence segmentation\n",
    "doc = nlp(user_input)\n",
    "results = []\n",
    "chat_history = [] \n",
    "\n",
    "for sent in doc.sents:\n",
    "    result = graph.invoke({\n",
    "        \"query\": sent.text.strip(),\n",
    "    })\n",
    "    results.append(result[\"output\"])\n",
    "    chat_history = result.get(\"chat_history\", chat_history)\n",
    "\n",
    "merge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant who summarizes multiple answers into one elegant, coherent reply for a user.\"),\n",
    "    (\"user\", \"Here are partial answers:\\n\\n{answers}\\n\\nPlease summarize them smoothly.\")\n",
    "])\n",
    "\n",
    "if len(results) == 1:\n",
    "    final_output = results[0]\n",
    "else:\n",
    "    final_output = llm.invoke(\n",
    "        merge_prompt.format_messages(answers=\"\\n\\n\".join(results))\n",
    "    ).content\n",
    "\n",
    "\n",
    "print(final_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
